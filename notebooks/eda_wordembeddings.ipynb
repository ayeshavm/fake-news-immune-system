{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce003812",
   "metadata": {},
   "source": [
    "## Fake News using Artificial Immune Systems \n",
    "\n",
    "### Data\n",
    "\n",
    "✅ politifact_real.csv → real articles from Politifact \n",
    "\n",
    "✅ politifact_fake.csv → fake articles from Politifact\n",
    "\n",
    "✅ gossipcop_real.csv → real articles from GossipCop\n",
    "\n",
    "✅ gossipcop_fake.csv → fake articles from GossipCop\n",
    "\n",
    "### 📚 [References](https://github.com/KaiDMML/FakeNewsNet)\n",
    "\n",
    "- Shu, K., Mahudeswaran, D., Wang, S., Lee, D., & Liu, H. (2018). **FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media.** *arXiv preprint arXiv:1809.01286*. [arXiv link](https://arxiv.org/abs/1809.01286)\n",
    "\n",
    "- Shu, K., Sliva, A., Wang, S., Tang, J., & Liu, H. (2017). **Fake News Detection on Social Media: A Data Mining Perspective.** *ACM SIGKDD Explorations Newsletter*, 19(1), 22–36. [DOI](https://doi.org/10.1145/3137597.3137600)\n",
    "\n",
    "- Shu, K., Wang, S., & Liu, H. (2017). **Exploiting Tri-Relationship for Fake News Detection.** *arXiv preprint arXiv:1712.07709*. [arXiv link](https://arxiv.org/abs/1712.07709)\n",
    "✅ Includes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f323bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026fc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real news shape: (624, 4)\n",
      "Fake news shape: (432, 4)\n",
      "\n",
      "Sample real news article:\n",
      "id                                             politifact14984\n",
      "news_url                             http://www.nfib-sbet.org/\n",
      "title              National Federation of Independent Business\n",
      "tweet_ids    967132259869487105\\t967164368768196609\\t967215...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Sample fake news article:\n",
      "id                                             politifact15014\n",
      "news_url             speedtalk.com/forum/viewtopic.php?t=51650\n",
      "title        BREAKING: First NFL Team Declares Bankruptcy O...\n",
      "tweet_ids    937349434668498944\\t937379378006282240\\t937380...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load real and fake from politifact\n",
    "basepath = \"/Users/ayeshamendoza/repos/fake-news-immune-system\"\n",
    "datapath = os.path.join(basepath, \"data/raw\")\n",
    "real = pd.read_csv(os.path.join(datapath, 'politifact_real.csv'))\n",
    "fake = pd.read_csv(os.path.join(datapath, 'politifact_fake.csv'))\n",
    "\n",
    "print(\"Real news shape:\", real.shape)\n",
    "print(\"Fake news shape:\", fake.shape)\n",
    "\n",
    "print(\"\\nSample real news article:\")\n",
    "print(real.iloc[0])\n",
    "\n",
    "print(\"\\nSample fake news article:\")\n",
    "print(fake.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e4e8f",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "In order to be able to use the text data in our Deep Learning models, we will need to convert the text data to numbers.  In order to do that the following pre-processing steps were done:\n",
    "\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- removing stopwords\n",
    "- removing punctuations\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources if not done\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define cleaning function\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb866da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)              # remove ellipsis\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text) # remove URLs\n",
    "    text = re.sub(r'\\$\\w*', '', text)                # remove $ mentions\n",
    "    text = re.sub(r'#', '', text)                    # remove hashtags\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # <-- remove punctuation\n",
    "\n",
    "    tokens = text.split()  # now safe to split on whitespace\n",
    "\n",
    "    cleaned_tokens = [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "basepath = \"/Users/ayeshamendoza/repos/fake-news-immune-system\"\n",
    "datapath = os.path.join(basepath, \"data/raw\")\n",
    "real = pd.read_csv(os.path.join(datapath, 'politifact_real.csv'))\n",
    "fake = pd.read_csv(os.path.join(datapath, 'politifact_fake.csv'))\n",
    "\n",
    "# Add label columns\n",
    "real['label'] = 'REAL'\n",
    "fake['label'] = 'FAKE'\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([real, fake], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33884fd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f400f15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-macosx_10_9_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from spacy) (2.2.5)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (4.0 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.5-cp310-cp310-macosx_10_9_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-macosx_10_9_x86_64.whl (41 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp310-cp310-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-macosx_10_9_x86_64.whl (132 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-macosx_10_9_x86_64.whl (636 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.0/636.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp310-cp310-macosx_10_9_x86_64.whl (894 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.7/894.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.3.0-cp310-cp310-macosx_10_9_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp310-cp310-macosx_10_9_universal2.whl (201 kB)\n",
      "Downloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-macosx_10_9_universal2.whl (14 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-macosx_10_9_x86_64.whl (192 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-macosx_10_9_x86_64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, MarkupSafe, marisa-trie, idna, cloudpathlib, charset-normalizer, certifi, catalogue, blis, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, language-data, jinja2, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.4.26 charset-normalizer-3.4.2 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.11.4 pydantic-core-2.33.2 requests-2.32.3 rich-14.0.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.15.3 typing-inspection-0.4.0 urllib3-2.4.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf6acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61db820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                         clean_text\n",
      "0  REAL                         nation feder independ busi\n",
      "1  REAL                              comment fayettevil nc\n",
      "2  REAL  romney make pitch hope close deal elect rocki ...\n",
      "3  REAL  democrat leader say hous democrat unit gop def...\n",
      "4  REAL                   budget unit state govern fy 2008\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df['clean_text'] = df['title'].fillna('')\n",
    "df['clean_text'] = df['clean_text'].apply(clean_text)\n",
    "# df['clean_text'] = df['title'].fillna('').apply(clean_text)\n",
    "\n",
    "# OPTIONAL: Save cleaned dataset\n",
    "df.to_csv('../data/processed/cleaned_articles.csv', index=False)\n",
    "\n",
    "# Preview cleaned text\n",
    "print(df[['label', 'clean_text']].head())\n",
    "\n",
    "article_texts = df['clean_text'].tolist()\n",
    "\n",
    "# # ✅ TF-IDF Vectorizer\n",
    "# vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3064b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings shape: (1056, 300)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "article_vectors = []\n",
    "for doc in nlp.pipe(article_texts, disable=[\"ner\", \"parser\"]):\n",
    "    article_vectors.append(doc.vector)\n",
    "\n",
    "article_vectors = np.array(article_vectors)\n",
    "print(\"✅ Embeddings shape:\", article_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bad246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 300 detectors in 1821 attempts (threshold=0.4, noise_std=0.05)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "import src.negative_selection\n",
    "import importlib\n",
    "importlib.reload(src.negative_selection)\n",
    "\n",
    "from src.negative_selection import generate_detectors, detect_anomaly\n",
    "# From your DataFrame\n",
    "# article_vectors, true_labels = embed_articles_from_df(df, text_col='text', label_col='label')\n",
    "label_map = {'REAL': 0, 'FAKE': 1}\n",
    "true_labels = df['label'].map(label_map).tolist()\n",
    "# Use only real news vectors for training\n",
    "num_real = sum(1 for label in true_labels if label == 0)\n",
    "self_matrix = article_vectors[:num_real]\n",
    "vector_dim = article_vectors.shape[1]\n",
    "\n",
    "# Generate detectors\n",
    "num_detectors = 300 #100\n",
    "detectors = generate_detectors(\n",
    "    num_detectors=num_detectors,\n",
    "    vector_dim=vector_dim,\n",
    "    self_matrix=self_matrix,\n",
    "    threshold=0.4,\n",
    "    noise_std=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c24f28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Min distance: 0.40016254506388793\n",
      "📏 Max distance: 1.0\n",
      "📊 Mean distance: 0.5693185149347891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "all_min_dists = []\n",
    "for article_vec in article_vectors:\n",
    "    distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "    all_min_dists.append(np.min(distances))\n",
    "\n",
    "print(\"🔎 Min distance:\", np.min(all_min_dists))\n",
    "print(\"📏 Max distance:\", np.max(all_min_dists))\n",
    "print(\"📊 Mean distance:\", np.mean(all_min_dists))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25592d7",
   "metadata": {},
   "source": [
    "✅ Your Detector-to-Article Distance Stats:\n",
    "Metric\tValue\tWhat It Means\n",
    "Min\t0.4001\tOne article is just barely triggering a detector at threshold = 0.4 — perfect edge case for detection!\n",
    "Mean\t0.59\tMost articles are comfortably distant — ideal for anomaly detection\n",
    "Max\t1.0\tSome articles are totally outside detector space, as expected\n",
    "\n",
    "🔥 What This Tells Us:\n",
    "Your detectors are nicely placed in embedding space\n",
    "\n",
    "Using a threshold of 0.4–0.6 should give actual detection coverage now\n",
    "\n",
    "You're ready to test the full prediction + evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c0f4465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.output_wrapper, .output {height:auto !important; max-height: none !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# This disables output scrolling in notebook cells\n",
    "display(HTML(\"<style>.output_wrapper, .output {height:auto !important; max-height: none !important;}</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f80f0d31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'savepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msavepath\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'savepath' is not defined"
     ]
    }
   ],
   "source": [
    "savepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f72cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "savepath = os.path.join(basepath, \"data/processed\")\n",
    "\n",
    "writepath = os.path.join(savepath, \"ais_results.txt\")\n",
    "thresholds = [0.5, 0.52, 0.54, 0.56, 0.58, 0.6]\n",
    "\n",
    "with open(writepath, \"w\") as f:\n",
    "    for t in thresholds:\n",
    "        predictions = []\n",
    "        for vec in article_vectors:\n",
    "            distances = cosine_distances(detectors, vec.reshape(1, -1)).flatten()\n",
    "            is_fake = np.any(distances < t)\n",
    "            predictions.append(int(is_fake))\n",
    "\n",
    "        f.write(f\"\\n🔍 Threshold = {t}\\n\")\n",
    "        f.write(str(confusion_matrix(true_labels, predictions)) + \"\\n\")\n",
    "        f.write(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "335edff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Threshold = 0.5\n",
      "[[448 176]\n",
      " [403  29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.53      0.72      0.61       624\n",
      "        Fake       0.14      0.07      0.09       432\n",
      "\n",
      "    accuracy                           0.45      1056\n",
      "   macro avg       0.33      0.39      0.35      1056\n",
      "weighted avg       0.37      0.45      0.40      1056\n",
      "\n",
      "\n",
      "🔍 Threshold = 0.52\n",
      "[[407 217]\n",
      " [374  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.52      0.65      0.58       624\n",
      "        Fake       0.21      0.13      0.16       432\n",
      "\n",
      "    accuracy                           0.44      1056\n",
      "   macro avg       0.37      0.39      0.37      1056\n",
      "weighted avg       0.39      0.44      0.41      1056\n",
      "\n",
      "\n",
      "🔍 Threshold = 0.54\n",
      "[[349 275]\n",
      " [335  97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.51      0.56      0.53       624\n",
      "        Fake       0.26      0.22      0.24       432\n",
      "\n",
      "    accuracy                           0.42      1056\n",
      "   macro avg       0.39      0.39      0.39      1056\n",
      "weighted avg       0.41      0.42      0.41      1056\n",
      "\n",
      "\n",
      "🔍 Threshold = 0.56\n",
      "[[295 329]\n",
      " [279 153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.51      0.47      0.49       624\n",
      "        Fake       0.32      0.35      0.33       432\n",
      "\n",
      "    accuracy                           0.42      1056\n",
      "   macro avg       0.42      0.41      0.41      1056\n",
      "weighted avg       0.43      0.42      0.43      1056\n",
      "\n",
      "\n",
      "🔍 Threshold = 0.58\n",
      "[[238 386]\n",
      " [207 225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.53      0.38      0.45       624\n",
      "        Fake       0.37      0.52      0.43       432\n",
      "\n",
      "    accuracy                           0.44      1056\n",
      "   macro avg       0.45      0.45      0.44      1056\n",
      "weighted avg       0.47      0.44      0.44      1056\n",
      "\n",
      "\n",
      "🔍 Threshold = 0.6\n",
      "[[197 427]\n",
      " [152 280]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.56      0.32      0.40       624\n",
      "        Fake       0.40      0.65      0.49       432\n",
      "\n",
      "    accuracy                           0.45      1056\n",
      "   macro avg       0.48      0.48      0.45      1056\n",
      "weighted avg       0.50      0.45      0.44      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "thresholds = [0.5, 0.52, 0.54, 0.56, 0.58, 0.6]\n",
    "# for t in [0.4, 0.45, 0.5, 0.55]:\n",
    "for t in thresholds:\n",
    "    predictions = []\n",
    "    for vec in article_vectors:\n",
    "        distances = cosine_distances(detectors, vec.reshape(1, -1)).flatten()\n",
    "        is_fake = np.any(distances < t)\n",
    "        predictions.append(int(is_fake))\n",
    "\n",
    "    print(f\"\\n🔍 Threshold = {t}\")\n",
    "    print(confusion_matrix(true_labels, predictions))\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# # Fit TF-IDF\n",
    "# # vectorizer = TfidfVectorizer(max_features=5000)  # limit vocab to top 5000 tokens\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     max_features=5000,\n",
    "#     token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b'\n",
    "# )\n",
    "# tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# # Vocab size\n",
    "# print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# # Preview first 20 tokens in vocab\n",
    "# print(\"\\nSample vocab tokens:\")\n",
    "# sample_tokens = list(vectorizer.vocabulary_.keys())[:20]\n",
    "# print(sample_tokens)\n",
    "\n",
    "# # Show shape\n",
    "# print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# # Show top tokens by IDF (most unique)\n",
    "# idf_scores = vectorizer.idf_\n",
    "# tokens_idf = sorted(zip(vectorizer.get_feature_names_out(), idf_scores), key=lambda x: -x[1])\n",
    "# print(\"\\nTop 10 tokens by IDF (most unique):\")\n",
    "# for token, idf in tokens_idf[:10]:\n",
    "#     print(f\"{token}: {idf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e57e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c14e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "savepath = os.path.join(basepath, \"data/processed\")\n",
    "df.to_csv(os.path.join(savepath, \"clean_articles.csv\"), index=False)\n",
    "\n",
    "sparse.save_npz(os.path.join(savepath, \"tfidf_matrix.npz\"), tfidf_matrix)\n",
    "\n",
    "# Save vocab\n",
    "import pickle\n",
    "with open(os.path.join(savepath,'tfidf_vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(vectorizer.vocabulary_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b410f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "import src.negative_selection\n",
    "import importlib\n",
    "importlib.reload(src.negative_selection)\n",
    "\n",
    "from src.negative_selection import generate_detectors\n",
    "from scipy import sparse\n",
    "\n",
    "# Load saved tfidf matrix\n",
    "self_matrix = sparse.load_npz(os.path.join(savepath, 'tfidf_matrix.npz'))\n",
    "\n",
    "threshold = 0.8  # tweak threshold as needed\n",
    "num_detectors = 100\n",
    "\n",
    "num_real = len(real)\n",
    "self_matrix = tfidf_matrix[:num_real]  # ONLY real news\n",
    "vector_dim = self_matrix.shape[1]\n",
    "\n",
    "# detectors = generate_detectors(num_detectors, vector_dim, self_matrix, threshold)\n",
    "# detectors = generate_detectors(200, vector_dim, self_matrix, threshold)\n",
    "detectors = generate_detectors(\n",
    "    num_detectors=100,\n",
    "    vector_dim=vector_dim,\n",
    "    self_matrix=self_matrix,\n",
    "    threshold=0.5,         # More realistic\n",
    "    noise_std=0.05         # Gives variation\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Generated {len(detectors)} detectors (requested {num_detectors})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d746b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detectors.shape)  # should be (100, vector_dim)\n",
    "print(detectors[0][:10])  # first 10 values of first detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "all_min_dists = []\n",
    "\n",
    "for article_vec in tfidf_matrix.toarray():  # or .A\n",
    "    distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "    all_min_dists.append(np.min(distances))\n",
    "\n",
    "# Basic stats\n",
    "print(\"Min distance:\", np.min(all_min_dists))\n",
    "print(\"Max distance:\", np.max(all_min_dists))\n",
    "print(\"Mean distance:\", np.mean(all_min_dists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad46ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = []\n",
    "for article_vec in tfidf_matrix.toarray():\n",
    "    distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "    is_fake = np.any(distances < 0.55)  # ← threshold here\n",
    "    predictions.append(int(is_fake))\n",
    "\n",
    "# Evaluate\n",
    "label_map = {'REAL': 0, 'FAKE': 1}\n",
    "true_labels = df['label'].map(label_map).tolist()\n",
    "print(confusion_matrix(true_labels, predictions))\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871de77",
   "metadata": {},
   "source": [
    "🔍 Let’s Break Down What’s Going On\n",
    "Confusion Matrix:\n",
    "\n",
    "\n",
    "[[509 115]   ← Real: 509 correct, 115 false positives (flagged as fake)\n",
    " [432   0]]   ← Fake: 432 fake articles, all missed ❌\n",
    "\n",
    "\n",
    "We're correctly identifying a good number of real articles (recall = 82%)\n",
    "\n",
    "But we're not catching any fake news at all — detectors didn’t fire on them\n",
    "\n",
    "Precision for \"Fake\" = 0, recall for \"Fake\" = 0 → F1 = 0\n",
    "\n",
    "💡 Diagnosis\n",
    "❓Possibility 1: Detectors are too similar to real, not close to fake\n",
    "We built detectors based on noise from real news\n",
    "\n",
    "If fake news vectors look too similar to real ones (in TF-IDF space), they slip through undetected\n",
    "\n",
    "❓Possibility 2: Threshold is too strict\n",
    "You used threshold = 0.55\n",
    "\n",
    "But we saw earlier that min distances start at ~0.50, and mean = 0.84\n",
    "\n",
    "Try lowering threshold to ~0.7 or even 0.75 to allow detectors to fire on fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.7, 0.75]:\n",
    "    predictions = []\n",
    "    for article_vec in tfidf_matrix.toarray():\n",
    "        distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "        is_fake = np.any(distances < t)\n",
    "        predictions.append(int(is_fake))\n",
    "\n",
    "    print(f\"\\n🔎 Threshold = {t}\")\n",
    "    print(confusion_matrix(true_labels, predictions))\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# sample_detector = detectors[0]\n",
    "# distances = np.linalg.norm(self_matrix.toarray() - sample_detector, axis=1)\n",
    "\n",
    "# plt.hist(distances, bins=30)\n",
    "# plt.xlabel('Distance to self')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distances from sample detector to self samples')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e86c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f517dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "\n",
    "import src.negative_selection\n",
    "import importlib\n",
    "importlib.reload(src.negative_selection)\n",
    "\n",
    "from src.negative_selection import detect_anomaly\n",
    "\n",
    "# # Pick sample article (convert sparse to dense row)\n",
    "# sample_article_vector = tfidf_matrix[0].toarray()[0]\n",
    "\n",
    "# result = detect_anomaly(sample_article_vector, detectors, threshold)\n",
    "\n",
    "# print(\"Article detected as FAKE\" if result else \"Article detected as REAL\")\n",
    "\n",
    "predictions = []\n",
    "threshold = 0.05\n",
    "\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    # Get article vector → convert sparse row to dense array\n",
    "    article_vector = tfidf_matrix[i].toarray()[0]\n",
    "    \n",
    "    # Run detection\n",
    "    detected = detect_anomaly(article_vector, detectors, threshold)\n",
    "    \n",
    "    # Map True/False → FAKE/REAL\n",
    "    predictions.append('FAKE' if detected else 'REAL')\n",
    "\n",
    "# Assign predictions to dataframe\n",
    "df['predicted_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(df['label'], predictions, target_names=[\"Real\", \"Fake\"]))\n",
    "print(confusion_matrix(df['label'], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (df['label'] == df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0851d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.02, 0.04, 0.08]:\n",
    "    preds = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        article_vector = tfidf_matrix[i].toarray()[0]\n",
    "        detected = detect_anomaly(article_vector, detectors, t)\n",
    "        preds.append('FAKE' if detected else 'REAL')\n",
    "    acc = (df['label'] == preds).mean()\n",
    "    print(f\"Threshold {t}: Accuracy {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f34427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256c271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-immune-system-pOZ85LOU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
