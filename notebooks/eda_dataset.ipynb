{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce003812",
   "metadata": {},
   "source": [
    "## Fake News using Artificial Immune Systems \n",
    "\n",
    "### Data\n",
    "\n",
    "âœ… politifact_real.csv â†’ real articles from Politifact \n",
    "\n",
    "âœ… politifact_fake.csv â†’ fake articles from Politifact\n",
    "\n",
    "âœ… gossipcop_real.csv â†’ real articles from GossipCop\n",
    "\n",
    "âœ… gossipcop_fake.csv â†’ fake articles from GossipCop\n",
    "\n",
    "### ðŸ“š [References](https://github.com/KaiDMML/FakeNewsNet)\n",
    "\n",
    "- Shu, K., Mahudeswaran, D., Wang, S., Lee, D., & Liu, H. (2018). **FakeNewsNet: A Data Repository with News Content, Social Context and Dynamic Information for Studying Fake News on Social Media.** *arXiv preprint arXiv:1809.01286*. [arXiv link](https://arxiv.org/abs/1809.01286)\n",
    "\n",
    "- Shu, K., Sliva, A., Wang, S., Tang, J., & Liu, H. (2017). **Fake News Detection on Social Media: A Data Mining Perspective.** *ACM SIGKDD Explorations Newsletter*, 19(1), 22â€“36. [DOI](https://doi.org/10.1145/3137597.3137600)\n",
    "\n",
    "- Shu, K., Wang, S., & Liu, H. (2017). **Exploiting Tri-Relationship for Fake News Detection.** *arXiv preprint arXiv:1712.07709*. [arXiv link](https://arxiv.org/abs/1712.07709)\n",
    "âœ… Includes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f323bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026fc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real news shape: (624, 4)\n",
      "Fake news shape: (432, 4)\n",
      "\n",
      "Sample real news article:\n",
      "id                                             politifact14984\n",
      "news_url                             http://www.nfib-sbet.org/\n",
      "title              National Federation of Independent Business\n",
      "tweet_ids    967132259869487105\\t967164368768196609\\t967215...\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Sample fake news article:\n",
      "id                                             politifact15014\n",
      "news_url             speedtalk.com/forum/viewtopic.php?t=51650\n",
      "title        BREAKING: First NFL Team Declares Bankruptcy O...\n",
      "tweet_ids    937349434668498944\\t937379378006282240\\t937380...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load real and fake from politifact\n",
    "basepath = \"/Users/ayeshamendoza/repos/fake-news-immune-system\"\n",
    "datapath = os.path.join(basepath, \"data/raw\")\n",
    "real = pd.read_csv(os.path.join(datapath, 'politifact_real.csv'))\n",
    "fake = pd.read_csv(os.path.join(datapath, 'politifact_fake.csv'))\n",
    "\n",
    "print(\"Real news shape:\", real.shape)\n",
    "print(\"Fake news shape:\", fake.shape)\n",
    "\n",
    "print(\"\\nSample real news article:\")\n",
    "print(real.iloc[0])\n",
    "\n",
    "print(\"\\nSample fake news article:\")\n",
    "print(fake.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e4e8f",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "In order to be able to use the text data in our Deep Learning models, we will need to convert the text data to numbers.  In order to do that the following pre-processing steps were done:\n",
    "\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- removing stopwords\n",
    "- removing punctuations\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download NLTK resources if not done\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Define cleaning function\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb866da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    stop_words = ENGLISH_STOP_WORDS\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)              # remove ellipsis\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text) # remove URLs\n",
    "    text = re.sub(r'\\$\\w*', '', text)                # remove $ mentions\n",
    "    text = re.sub(r'#', '', text)                    # remove hashtags\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # <-- remove punctuation\n",
    "\n",
    "    tokens = text.split()  # now safe to split on whitespace\n",
    "\n",
    "    cleaned_tokens = [\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words\n",
    "    ]\n",
    "\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "basepath = \"/Users/ayeshamendoza/repos/fake-news-immune-system\"\n",
    "datapath = os.path.join(basepath, \"data/raw\")\n",
    "real = pd.read_csv(os.path.join(datapath, 'politifact_real.csv'))\n",
    "fake = pd.read_csv(os.path.join(datapath, 'politifact_fake.csv'))\n",
    "\n",
    "# Add label columns\n",
    "real['label'] = 'REAL'\n",
    "fake['label'] = 'FAKE'\n",
    "\n",
    "# Combine datasets\n",
    "df = pd.concat([real, fake], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33884fd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61db820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                         clean_text\n",
      "0  REAL                         nation feder independ busi\n",
      "1  REAL                              comment fayettevil nc\n",
      "2  REAL  romney make pitch hope close deal elect rocki ...\n",
      "3  REAL  democrat leader say hous democrat unit gop def...\n",
      "4  REAL                   budget unit state govern fy 2008\n",
      "TF-IDF matrix shape: (1056, 2740)\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df['clean_text'] = df['title'].fillna('')\n",
    "df['clean_text'] = df['clean_text'].apply(clean_text)\n",
    "# df['clean_text'] = df['title'].fillna('').apply(clean_text)\n",
    "\n",
    "# OPTIONAL: Save cleaned dataset\n",
    "df.to_csv('../data/processed/cleaned_articles.csv', index=False)\n",
    "\n",
    "# Preview cleaned text\n",
    "print(df[['label', 'clean_text']].head())\n",
    "\n",
    "# âœ… TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1f897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2610\n",
      "\n",
      "Sample vocab tokens:\n",
      "['nation', 'feder', 'independ', 'busi', 'comment', 'fayettevil', 'nc', 'romney', 'make', 'pitch', 'hope', 'close', 'deal', 'elect', 'rocki', 'mountain', 'news', 'democrat', 'leader', 'say']\n",
      "\n",
      "TF-IDF matrix shape: (1056, 2610)\n",
      "\n",
      "Top 10 tokens by IDF (most unique):\n",
      "abandon: 7.27\n",
      "abedin: 7.27\n",
      "abid: 7.27\n",
      "abortion: 7.27\n",
      "absorb: 7.27\n",
      "abus: 7.27\n",
      "achiev: 7.27\n",
      "acquir: 7.27\n",
      "activ: 7.27\n",
      "actuari: 7.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Fit TF-IDF\n",
    "# vectorizer = TfidfVectorizer(max_features=5000)  # limit vocab to top 5000 tokens\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b'\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Vocab size\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Preview first 20 tokens in vocab\n",
    "print(\"\\nSample vocab tokens:\")\n",
    "sample_tokens = list(vectorizer.vocabulary_.keys())[:20]\n",
    "print(sample_tokens)\n",
    "\n",
    "# Show shape\n",
    "print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Show top tokens by IDF (most unique)\n",
    "idf_scores = vectorizer.idf_\n",
    "tokens_idf = sorted(zip(vectorizer.get_feature_names_out(), idf_scores), key=lambda x: -x[1])\n",
    "print(\"\\nTop 10 tokens by IDF (most unique):\")\n",
    "for token, idf in tokens_idf[:10]:\n",
    "    print(f\"{token}: {idf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e57e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ayeshamendoza/repos/fake-news-immune-system'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c14e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "savepath = os.path.join(basepath, \"data/processed\")\n",
    "df.to_csv(os.path.join(savepath, \"clean_articles.csv\"), index=False)\n",
    "\n",
    "sparse.save_npz(os.path.join(savepath, \"tfidf_matrix.npz\"), tfidf_matrix)\n",
    "\n",
    "# Save vocab\n",
    "import pickle\n",
    "with open(os.path.join(savepath,'tfidf_vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(vectorizer.vocabulary_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44b410f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated 100 detectors in 123 attempts (threshold=0.5, noise_std=0.05)\n",
      "Generated 100 detectors (requested 100)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "import src.negative_selection\n",
    "import importlib\n",
    "importlib.reload(src.negative_selection)\n",
    "\n",
    "from src.negative_selection import generate_detectors\n",
    "from scipy import sparse\n",
    "\n",
    "# Load saved tfidf matrix\n",
    "self_matrix = sparse.load_npz(os.path.join(savepath, 'tfidf_matrix.npz'))\n",
    "\n",
    "threshold = 0.8  # tweak threshold as needed\n",
    "num_detectors = 100\n",
    "\n",
    "num_real = len(real)\n",
    "self_matrix = tfidf_matrix[:num_real]  # ONLY real news\n",
    "vector_dim = self_matrix.shape[1]\n",
    "\n",
    "# detectors = generate_detectors(num_detectors, vector_dim, self_matrix, threshold)\n",
    "# detectors = generate_detectors(200, vector_dim, self_matrix, threshold)\n",
    "detectors = generate_detectors(\n",
    "    num_detectors=100,\n",
    "    vector_dim=vector_dim,\n",
    "    self_matrix=self_matrix,\n",
    "    threshold=0.5,         # More realistic\n",
    "    noise_std=0.05         # Gives variation\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Generated {len(detectors)} detectors (requested {num_detectors})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d746b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2610)\n",
      "[0.         0.         0.         0.03721172 0.05465079 0.\n",
      " 0.03827826 0.00081696 0.0386581  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(detectors.shape)  # should be (100, vector_dim)\n",
    "print(detectors[0][:10])  # first 10 values of first detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce12efac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance: 0.5020335620804497\n",
      "Max distance: 1.0\n",
      "Mean distance: 0.8460125867323083\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "all_min_dists = []\n",
    "\n",
    "for article_vec in tfidf_matrix.toarray():  # or .A\n",
    "    distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "    all_min_dists.append(np.min(distances))\n",
    "\n",
    "# Basic stats\n",
    "print(\"Min distance:\", np.min(all_min_dists))\n",
    "print(\"Max distance:\", np.max(all_min_dists))\n",
    "print(\"Mean distance:\", np.mean(all_min_dists))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6eff3836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REAL', 'REAL', 'REAL', 'REAL', 'REAL'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1eab0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>label_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politifact14984</td>\n",
       "      <td>http://www.nfib-sbet.org/</td>\n",
       "      <td>National Federation of Independent Business</td>\n",
       "      <td>967132259869487105\\t967164368768196609\\t967215...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>nation feder independ busi</td>\n",
       "      <td>REAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politifact12944</td>\n",
       "      <td>http://www.cq.com/doc/newsmakertranscripts-494...</td>\n",
       "      <td>comments in Fayetteville NC</td>\n",
       "      <td>942953459\\t8980098198\\t16253717352\\t1668513250...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>comment fayettevil nc</td>\n",
       "      <td>REAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politifact333</td>\n",
       "      <td>https://web.archive.org/web/20080204072132/htt...</td>\n",
       "      <td>Romney makes pitch, hoping to close deal : Ele...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REAL</td>\n",
       "      <td>romney make pitch hope close deal elect rocki ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politifact4358</td>\n",
       "      <td>https://web.archive.org/web/20110811143753/htt...</td>\n",
       "      <td>Democratic Leaders Say House Democrats Are Uni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REAL</td>\n",
       "      <td>democrat leader say hous democrat unit gop def...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politifact779</td>\n",
       "      <td>https://web.archive.org/web/20070820164107/htt...</td>\n",
       "      <td>Budget of the United States Government, FY 2008</td>\n",
       "      <td>89804710374154240\\t91270460595109888\\t96039619...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>budget unit state govern fy 2008</td>\n",
       "      <td>REAL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                           news_url  \\\n",
       "0  politifact14984                          http://www.nfib-sbet.org/   \n",
       "1  politifact12944  http://www.cq.com/doc/newsmakertranscripts-494...   \n",
       "2    politifact333  https://web.archive.org/web/20080204072132/htt...   \n",
       "3   politifact4358  https://web.archive.org/web/20110811143753/htt...   \n",
       "4    politifact779  https://web.archive.org/web/20070820164107/htt...   \n",
       "\n",
       "                                               title  \\\n",
       "0        National Federation of Independent Business   \n",
       "1                        comments in Fayetteville NC   \n",
       "2  Romney makes pitch, hoping to close deal : Ele...   \n",
       "3  Democratic Leaders Say House Democrats Are Uni...   \n",
       "4    Budget of the United States Government, FY 2008   \n",
       "\n",
       "                                           tweet_ids label  \\\n",
       "0  967132259869487105\\t967164368768196609\\t967215...  REAL   \n",
       "1  942953459\\t8980098198\\t16253717352\\t1668513250...  REAL   \n",
       "2                                                NaN  REAL   \n",
       "3                                                NaN  REAL   \n",
       "4  89804710374154240\\t91270460595109888\\t96039619...  REAL   \n",
       "\n",
       "                                          clean_text predicted_label  \\\n",
       "0                         nation feder independ busi            REAL   \n",
       "1                              comment fayettevil nc            REAL   \n",
       "2  romney make pitch hope close deal elect rocki ...            REAL   \n",
       "3  democrat leader say hous democrat unit gop def...            REAL   \n",
       "4                   budget unit state govern fy 2008            REAL   \n",
       "\n",
       "   label_int  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {'REAL': 0, 'FAKE': 1}\n",
    "df['label_int'] = df['label'].map(label_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dad46ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[509 115]\n",
      " [432   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.54      0.82      0.65       624\n",
      "        Fake       0.00      0.00      0.00       432\n",
      "\n",
      "    accuracy                           0.48      1056\n",
      "   macro avg       0.27      0.41      0.33      1056\n",
      "weighted avg       0.32      0.48      0.38      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = []\n",
    "for article_vec in tfidf_matrix.toarray():\n",
    "    distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "    is_fake = np.any(distances < 0.55)  # â† threshold here\n",
    "    predictions.append(int(is_fake))\n",
    "\n",
    "# Evaluate\n",
    "true_labels = df['label_int'].values\n",
    "print(confusion_matrix(true_labels, predictions))\n",
    "print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871de77",
   "metadata": {},
   "source": [
    "ðŸ” Letâ€™s Break Down Whatâ€™s Going On\n",
    "Confusion Matrix:\n",
    "\n",
    "\n",
    "[[509 115]   â† Real: 509 correct, 115 false positives (flagged as fake)\n",
    " [432   0]]   â† Fake: 432 fake articles, all missed âŒ\n",
    "\n",
    "\n",
    "We're correctly identifying a good number of real articles (recall = 82%)\n",
    "\n",
    "But we're not catching any fake news at all â€” detectors didnâ€™t fire on them\n",
    "\n",
    "Precision for \"Fake\" = 0, recall for \"Fake\" = 0 â†’ F1 = 0\n",
    "\n",
    "ðŸ’¡ Diagnosis\n",
    "â“Possibility 1: Detectors are too similar to real, not close to fake\n",
    "We built detectors based on noise from real news\n",
    "\n",
    "If fake news vectors look too similar to real ones (in TF-IDF space), they slip through undetected\n",
    "\n",
    "â“Possibility 2: Threshold is too strict\n",
    "You used threshold = 0.55\n",
    "\n",
    "But we saw earlier that min distances start at ~0.50, and mean = 0.84\n",
    "\n",
    "Try lowering threshold to ~0.7 or even 0.75 to allow detectors to fire on fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "636bf3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Threshold = 0.7\n",
      "[[491 133]\n",
      " [431   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.53      0.79      0.64       624\n",
      "        Fake       0.01      0.00      0.00       432\n",
      "\n",
      "    accuracy                           0.47      1056\n",
      "   macro avg       0.27      0.39      0.32      1056\n",
      "weighted avg       0.32      0.47      0.38      1056\n",
      "\n",
      "\n",
      "ðŸ”Ž Threshold = 0.75\n",
      "[[477 147]\n",
      " [430   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.53      0.76      0.62       624\n",
      "        Fake       0.01      0.00      0.01       432\n",
      "\n",
      "    accuracy                           0.45      1056\n",
      "   macro avg       0.27      0.38      0.32      1056\n",
      "weighted avg       0.32      0.45      0.37      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in [0.7, 0.75]:\n",
    "    predictions = []\n",
    "    for article_vec in tfidf_matrix.toarray():\n",
    "        distances = cosine_distances(detectors, article_vec.reshape(1, -1)).flatten()\n",
    "        is_fake = np.any(distances < t)\n",
    "        predictions.append(int(is_fake))\n",
    "\n",
    "    print(f\"\\nðŸ”Ž Threshold = {t}\")\n",
    "    print(confusion_matrix(true_labels, predictions))\n",
    "    print(classification_report(true_labels, predictions, target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9c5592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# sample_detector = detectors[0]\n",
    "# distances = np.linalg.norm(self_matrix.toarray() - sample_detector, axis=1)\n",
    "\n",
    "# plt.hist(distances, bins=30)\n",
    "# plt.xlabel('Distance to self')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Distances from sample detector to self samples')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e86c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8f517dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') \n",
    "\n",
    "import src.negative_selection\n",
    "import importlib\n",
    "importlib.reload(src.negative_selection)\n",
    "\n",
    "from src.negative_selection import detect_anomaly\n",
    "\n",
    "# # Pick sample article (convert sparse to dense row)\n",
    "# sample_article_vector = tfidf_matrix[0].toarray()[0]\n",
    "\n",
    "# result = detect_anomaly(sample_article_vector, detectors, threshold)\n",
    "\n",
    "# print(\"Article detected as FAKE\" if result else \"Article detected as REAL\")\n",
    "\n",
    "predictions = []\n",
    "threshold = 0.05\n",
    "\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    # Get article vector â†’ convert sparse row to dense array\n",
    "    article_vector = tfidf_matrix[i].toarray()[0]\n",
    "    \n",
    "    # Run detection\n",
    "    detected = detect_anomaly(article_vector, detectors, threshold)\n",
    "    \n",
    "    # Map True/False â†’ FAKE/REAL\n",
    "    predictions.append('FAKE' if detected else 'REAL')\n",
    "\n",
    "# Assign predictions to dataframe\n",
    "df['predicted_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c5e3c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.00      0.00      0.00       432\n",
      "        Fake       0.59      1.00      0.74       624\n",
      "\n",
      "    accuracy                           0.59      1056\n",
      "   macro avg       0.30      0.50      0.37      1056\n",
      "weighted avg       0.35      0.59      0.44      1056\n",
      "\n",
      "[[  0 432]\n",
      " [  0 624]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ayeshamendoza/Library/Caches/pypoetry/virtualenvs/fake-news-immune-system-pOZ85LOU-py3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(df['label'], predictions, target_names=[\"Real\", \"Fake\"]))\n",
    "print(confusion_matrix(df['label'], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (df['label'] == df['predicted_label']).mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0851d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 0.02: Accuracy 59.09%\n",
      "Threshold 0.04: Accuracy 59.09%\n",
      "Threshold 0.08: Accuracy 59.09%\n"
     ]
    }
   ],
   "source": [
    "for t in [0.02, 0.04, 0.08]:\n",
    "    preds = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        article_vector = tfidf_matrix[i].toarray()[0]\n",
    "        detected = detect_anomaly(article_vector, detectors, t)\n",
    "        preds.append('FAKE' if detected else 'REAL')\n",
    "    acc = (df['label'] == preds).mean()\n",
    "    print(f\"Threshold {t}: Accuracy {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f34427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min distance: 0.8795570857692714\n",
      "Max distance: 1.0\n",
      "Mean distance: 0.9279772336929644\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256c271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-immune-system-pOZ85LOU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
